apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-dags-mlops
data:
  data_pipeline.py: |
    import os
    from datetime import datetime, timedelta
    import requests
    import pandas as pd
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from airflow.providers.postgres.operators.postgres import PostgresOperator
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    from psycopg2.extras import execute_values
    from sqlalchemy import create_engine, text
    from airflow.exceptions import AirflowSkipException
    from sklearn.model_selection import train_test_split

    BATCH_SIZE = 1500
    RAW_DB_URI = os.getenv("RAW_DB_CONN")
    CLEAN_DB_URI = os.getenv("CLEAN_DB_CONN")  
    API_URL = "http://10.43.101.108:80/data?group_number=6&day=Wednesday"
    API_URL_reset = "http://10.43.101.108:80/restart_data_generation?group_number=6&day=Wednesday"
    SCHEMA_RAW     = "raw_data"
    SCHEMA_CLEAN   = "clean_data"
    TABLE_NAME = "raw_data_init"
    TABLE_NAME_CLEAN = "clean_data_init"

    default_args = {
        "owner": "airflow",
        "retries": 1,
    }

    with DAG(
        dag_id="data_pipeline",
        default_args=default_args,
        start_date=datetime(2025, 5, 1),
        schedule_interval="@hourly",
        catchup=False,
        tags=["raw","ingestion","dataSource"],
    ) as dag:

        def create_schema_raw():
            """Crea los esquema si no existe."""
            engine = create_engine(RAW_DB_URI)
            ddl = f"CREATE SCHEMA IF NOT EXISTS {SCHEMA_RAW};"
            with engine.begin() as conn:
                conn.execute(text(ddl))

        def create_table_raw():
            """Crea tabla si no existen."""
            engine = create_engine(RAW_DB_URI)
            ddl = f"""
            CREATE SCHEMA IF NOT EXISTS {SCHEMA_RAW};
            CREATE TABLE IF NOT EXISTS {SCHEMA_RAW}.{TABLE_NAME} (
                brokered_by     NUMERIC,
                status          VARCHAR(50),
                price           NUMERIC,
                bed             NUMERIC,
                bath            NUMERIC,
                acre_lot        NUMERIC,
                street          VARCHAR(200),
                city            VARCHAR(100),
                state           VARCHAR(100),
                zip_code        VARCHAR(20),
                house_size      NUMERIC,
                prev_sold_date  DATE,
                load_date       TIMESTAMP NOT NULL
            );
            """
            with engine.begin() as conn:
                conn.execute(text(ddl))
        
        def load_raw_batch():
            """
            1) Llama a la API y obtiene el JSON.
            2) Construye un DataFrame solo con payload["data"].
            3) Abre una conexión psycopg2 vía engine.raw_connection()
               y vuela el DataFrame en batch con execute_values.
            """
            # Traer datos de la API (timeout 5 min)
            try:
                 resp = requests.get(API_URL, timeout=300)
                 resp.raise_for_status()
            except requests.exceptions.HTTPError as e:
                 if e.response is not None and e.response.status_code == 400:
                     # fin normal: no más datos que recolectar
                     raise AirflowSkipException("API devolvió 400 – ya no hay datos nuevos")
                 else:
                     raise
            payload = resp.json()

            records = payload.get("data", [])
            if not records:
                return

            df = pd.DataFrame(records)
            df["load_date"] = datetime.utcnow()

            engine = create_engine(RAW_DB_URI)
            raw_conn = engine.raw_connection()
            try:
                cur = raw_conn.cursor()
                cols = list(df.columns)
                insert_sql = f"""
                    INSERT INTO {SCHEMA_RAW}.{TABLE_NAME} ({','.join(cols)})
                    VALUES %s
                """
                values = [
                    tuple(row) for row in df[cols].itertuples(index=False, name=None)
                ]
                execute_values(cur, insert_sql, values, page_size=BATCH_SIZE)
                raw_conn.commit()
            finally:
                cur.close()
                raw_conn.close()

        def create_schema_clean():
            engine = create_engine(CLEAN_DB_URI)
            ddl = f"CREATE SCHEMA IF NOT EXISTS {SCHEMA_CLEAN};"
            with engine.begin() as conn:
                conn.execute(text(ddl))
        
        def create_table_clean():
            """Crea la tabla clean_data.clean_data_init si no existe."""
            engine = create_engine(CLEAN_DB_URI)
            ddl = f"""
            CREATE SCHEMA IF NOT EXISTS {SCHEMA_CLEAN};
            CREATE TABLE IF NOT EXISTS {SCHEMA_CLEAN}.{TABLE_NAME_CLEAN} (
                brokered_by     NUMERIC,
                status          VARCHAR(50),
                price           NUMERIC,
                bed             NUMERIC,
                bath            NUMERIC,
                acre_lot        NUMERIC,
                street          VARCHAR(200),
                city            VARCHAR(100),
                state          VARCHAR(100),
                zip_code        VARCHAR(20),
                house_size      NUMERIC,
                prev_sold_date  DATE,
                load_date       TIMESTAMP NOT NULL,
                split           VARCHAR(10) NOT NULL
            );
            """
            with engine.begin() as conn:
                conn.execute(text(ddl))

        def transform_and_load_clean():
            """Lee raw, filtra por load_date nuevo, limpia, particiona y carga en clean_data."""
            engine_c = create_engine(CLEAN_DB_URI)
            # 1) última fecha procesada en clean_data
            with engine_c.connect() as conn_c:
                result = conn_c.execute(text(f"""
                    SELECT MAX(load_date) AS maxd
                      FROM {SCHEMA_CLEAN}.{TABLE_NAME_CLEAN}
                """))
                max_date = result.scalar()

            engine_r = create_engine(RAW_DB_URI)
            # 2) leo sólo raws nuevos, usando raw_connection para pandas
            raw_conn = engine_r.raw_connection()
            try:
                if max_date:
                    df = pd.read_sql_query(
                        f"SELECT * FROM {SCHEMA_RAW}.{TABLE_NAME} WHERE load_date > %s",
                        con=raw_conn,
                        params=[max_date],
                    )
                else:
                    df = pd.read_sql_query(
                        f"SELECT * FROM {SCHEMA_RAW}.{TABLE_NAME}",
                        con=raw_conn,
                    )
            finally:
                raw_conn.close()

            if df.empty:
                return

            # 3) limpieza avanzada
            for c in ["street", "city", "state", "status"]:
                df[c] = df[c].astype(str).str.strip().str.lower()
            df["zip_code"] = df["zip_code"].astype(str).str.replace(r"\D+", "", regex=True)
            df = df[
                (df["price"] >= 0) &
                (df["house_size"] >= 0) &
                (df["acre_lot"] >= 0) &
                (pd.to_datetime(df["prev_sold_date"], errors="coerce") <= df["load_date"])
            ]

            # 4) partición train/test
            df["split"] = "train"
            _, test_idx = train_test_split(df.index, test_size=0.3, random_state=42)
            df.loc[test_idx, "split"] = "test"

            # 5) vuelco en batches al clean_data
            conn_c = engine_c.raw_connection()
            try:
                cur = conn_c.cursor()
                cols = list(df.columns)
                insert_sql = f"""
                    INSERT INTO {SCHEMA_CLEAN}.{TABLE_NAME_CLEAN}
                    ({','.join(cols)}) VALUES %s
                """
                values = [tuple(r) for r in df[cols].itertuples(index=False, name=None)]
                execute_values(cur, insert_sql, values, page_size=BATCH_SIZE)
                conn_c.commit()
            finally:
                cur.close()
                conn_c.close()

        t1 = PythonOperator(
            task_id="create_schema_raw",
            python_callable=create_schema_raw,
        )

        t2 = PythonOperator(
            task_id="create_table_raw",
            python_callable=create_table_raw,
        )

        t3 = PythonOperator(
            task_id="load_raw_batch",
            python_callable=load_raw_batch,
            execution_timeout=timedelta(minutes=5),
        )

        t4 = PythonOperator(
            task_id="create_schema_clean",
            python_callable=create_schema_clean,
        )
        
        t5 = PythonOperator(
            task_id="create_table_clean",
            python_callable=create_table_clean,
        )

        t6 = PythonOperator(
            task_id="transform_and_load_clean",
            python_callable=transform_and_load_clean,
            execution_timeout=timedelta(minutes=5),
        )

        t1 >> t2 >> t3 >> t4 >> t5 >> t6 

  modeling_pipeline.py: |
    import os
    from datetime import datetime, timedelta

    import pandas as pd
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from airflow.sensors.external_task import ExternalTaskSensor
    from airflow.exceptions import AirflowSkipException
    from sqlalchemy import create_engine, text
    import mlflow
    from mlflow.tracking import MlflowClient
    from mlflow import sklearn as mlflow_sklearn
    from sklearn.pipeline import Pipeline
    from sklearn.linear_model import GammaRegressor
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from sklearn.compose import ColumnTransformer
    from sklearn.preprocessing import OneHotEncoder, StandardScaler
    import shap

    # ─── Configuración ─────────────────────────────────────────────────────────────
    CLEAN_DB_URI    = os.getenv("CLEAN_DB_CONN")
    SCHEMA_CLEAN    = "clean_data"
    TABLE_CLEAN     = "clean_data_init"
    EXPERIMENT_NAME = "modeling_pipeline"
    SHARED_TMP      = "/opt/airflow/dags/tmp"
    MAX_SHAP        = 50000

    os.makedirs(SHARED_TMP, exist_ok=True)
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI"))

    default_args = {
        "owner": "airflow",
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    }

    # ─── DAG ───────────────────────────────────────────────────────────────────────
    with DAG(
        dag_id="modeling_pipeline",
        default_args=default_args,
        start_date=datetime(2025, 5, 1),
        schedule_interval="@hourly",
        catchup=False,
        tags=["modeling","mlflow"],
    ) as dag:

        wait_for_clean = ExternalTaskSensor(
            task_id="wait_for_clean_data",
            external_dag_id="data_pipeline",
            external_task_id="transform_and_load_clean",
            mode="reschedule",
            poke_interval=30,
            timeout=5*60,
        )

        def ensure_history_table_fn():
            engine = create_engine(CLEAN_DB_URI)
            with engine.begin() as conn:
                conn.execute(text(f"""
                    CREATE TABLE IF NOT EXISTS {SCHEMA_CLEAN}.model_history (
                        id             SERIAL PRIMARY KEY,
                        experiment_id  BIGINT    NOT NULL,
                        model_name     TEXT       NOT NULL,
                        run_id         TEXT       NOT NULL,
                        run_name       TEXT      NOT NULL,
                        model_version  INTEGER    NOT NULL,
                        new_mse        DOUBLE PRECISION,
                        new_rmse       DOUBLE PRECISION,
                        new_mae        DOUBLE PRECISION,
                        new_r2         DOUBLE PRECISION,
                        prod_mse       DOUBLE PRECISION,
                        prod_rmse      DOUBLE PRECISION,
                        prod_mae       DOUBLE PRECISION,
                        prod_r2        DOUBLE PRECISION,
                        promoted       BOOLEAN    NOT NULL,
                        shap_uri       TEXT,
                        trained_at     TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT NOW()
                    );
                """))

        ensure_history_table = PythonOperator(
            task_id="ensure_history_table",
            python_callable=ensure_history_table_fn,
        )

        def extract_data_fn():
            engine   = create_engine(CLEAN_DB_URI)
            raw_conn = engine.raw_connection()
            try:
                df = pd.read_sql_query(f"SELECT * FROM {SCHEMA_CLEAN}.{TABLE_CLEAN}", con=raw_conn)
            finally:
                raw_conn.close()

            if df.empty:
                raise AirflowSkipException("No hay datos en clean_data_init para extraer")

            df_train = df[df["split"]=="train"].drop(columns=["split"])
            df_test  = df[df["split"]=="test"].drop(columns=["split"])
            df_train.to_csv(f"{SHARED_TMP}/train.csv", index=False)
            df_test .to_csv(f"{SHARED_TMP}/test.csv",  index=False)
            print(f"✅ Extracted {len(df)} rows → {len(df_train)} train / {len(df_test)} test")

        extract_data = PythonOperator(
            task_id="extract_data",
            python_callable=extract_data_fn,
        )

        def train_and_log_fn(ti):
            mlflow.set_experiment(EXPERIMENT_NAME)
            with mlflow.start_run() as run:
                df_train = pd.read_csv(f"{SHARED_TMP}/train.csv")
                FEATURES = [
                    "brokered_by", "status", "bed", "bath", "acre_lot",
                    "street", "city", "state", "zip_code", "house_size", "prev_sold_date"
                ]
                for c in ["street", "city", "state", "status", "zip_code"]:
                    if c in df_train.columns:
                        df_train[c] = df_train[c].astype(str)
                print("Dtypes en X_train antes de entrenar:")
                print(df_train.dtypes)
                print("Ejemplo de valores de street:", df_train['street'].head(10))
                y_train = df_train.pop("price")
                X_train = df_train[[col for col in FEATURES if col in df_train.columns]].copy()
                missing = [c for c in FEATURES if c not in X_train.columns]
                if missing:
                    print(f"⚠️  Warning: faltan columnas esperadas en los datos: {missing}")
                import json
                with open(f"{SHARED_TMP}/features.json", "w") as f:
                    json.dump(FEATURES, f)
                mlflow.log_artifact(f"{SHARED_TMP}/features.json", artifact_path="features")
                assert not X_train.isnull().any().any(), "Hay NaN en X_train"
                assert not y_train.isnull().any(), "Hay NaN en y_train"
                cats = X_train.select_dtypes(include=["object","category"]).columns.tolist()
                low_card = [c for c in cats if df_train[c].nunique()<=8]
                high_card= [c for c in cats if df_train[c].nunique()>8]
                if high_card:
                    print(f"Descartando cat cols alta cardinalidad: {high_card}")
                ti.xcom_push("high_card", high_card)
                X_train = X_train.drop(columns=high_card)
                num_cols = [c for c in X_train.columns if c not in low_card]
                print(f"X_train shape: {X_train.shape}, num_cols: {num_cols}, low_card: {low_card}")
                preproc = ColumnTransformer([
                    ("cat", OneHotEncoder(handle_unknown="ignore", sparse=True), low_card),
                    ("num", StandardScaler(), num_cols),
                ], remainder="drop")
                X_train_trans = preproc.fit_transform(X_train)
                assert X_train_trans.shape[0] == X_train.shape[0]
                pipe = Pipeline([
                    ("preproc", preproc),
                    ("reg", GammaRegressor(max_iter=200))
                ])
                pipe.fit(X_train, y_train)
                final_features = list(X_train.columns)
                with open(f"{SHARED_TMP}/final_features.json", "w") as f:
                    json.dump(final_features, f)
                mlflow.log_artifact(f"{SHARED_TMP}/final_features.json", artifact_path="features")
                mlflow_sklearn.log_model(pipe, "model", registered_model_name="my_model")
                df_test = pd.read_csv(f"{SHARED_TMP}/test.csv")
                for c in ["street", "city", "state", "status", "zip_code"]:
                    if c in df_test.columns:
                        df_test[c] = df_test[c].astype(str)
                X_test = df_test[[col for col in FEATURES if col in df_test.columns]].copy()
                y_test = df_test["price"]
                X_test = X_test.drop(columns=high_card, errors="ignore")
                preds = pipe.predict(X_test)
                mse = mean_squared_error(y_test, preds)
                rmse = mse ** 0.5
                mae = mean_absolute_error(y_test, preds)
                r2 = r2_score(y_test, preds)
                run_name = run.data.tags.get("mlflow.runName", None)
                ti.xcom_push("run_name", run_name)
                for k, v in {"mse": mse, "rmse": rmse, "mae": mae, "r2": r2}.items():
                    mlflow.log_metric(k, v)

        train_and_log = PythonOperator(
            task_id="train_and_log",
            python_callable=train_and_log_fn,
        )

        def evaluate_and_promote_fn(ti):
            client = MlflowClient()
            exp    = client.get_experiment_by_name(EXPERIMENT_NAME)
            run    = client.search_runs([exp.experiment_id], order_by=["attributes.start_time desc"], max_results=1)[0]

            mv = int(client.search_model_versions(f"name='my_model' and run_id='{run.info.run_id}'")[0].version)

            metrics = {k: run.data.metrics[k] for k in ["mse","rmse","mae","r2"]}
            df_train = pd.read_csv(f"{SHARED_TMP}/train.csv")
            high_card = [c for c in df_train.select_dtypes(include=["object","category"]).columns if df_train[c].nunique()>8]

            prod = client.get_latest_versions("my_model", stages=["Production"])
            if prod:
                prod_pipe = mlflow.sklearn.load_model(prod[0].source)
                df_test  = pd.read_csv(f"{SHARED_TMP}/test.csv")
                df_test  = df_test.drop(columns=["price"]+high_card, errors="ignore")
                y_test   = pd.read_csv(f"{SHARED_TMP}/test.csv")["price"]
                p2       = prod_pipe.predict(df_test)
                prod_metrics = {
                    "prod_mse":  mean_squared_error(y_test, p2),
                    "prod_rmse": None,
                    "prod_mae":  mean_absolute_error(y_test, p2),
                    "prod_r2":   r2_score(y_test, p2)
                }
            else:
                prod_metrics = {"prod_mse":None,"prod_rmse":None,"prod_mae":float("inf"),"prod_r2":float("-inf")}

            promoted = metrics["mae"] < prod_metrics["prod_mae"]
            if promoted:
                client.transition_model_version_stage(
                    name="my_model",
                    version=mv,
                    stage="Production",
                    archive_existing_versions=True
                )
            
            promoted = int(promoted)
            ti.xcom_push("cmp", {**metrics, **prod_metrics, "exp_id": exp.experiment_id,
                                 "run_id": run.info.run_id, "mv": mv, "promoted": promoted})

        evaluate_and_promote = PythonOperator(
            task_id="evaluate_and_promote",
            python_callable=evaluate_and_promote_fn,
        )

        def compute_shap_fn(ti):
            cmp = ti.xcom_pull(task_ids="evaluate_and_promote", key="cmp")
            if not cmp:
                raise AirflowSkipException("No se encontró información de métricas (cmp) para compute_shap")

            high_card = ti.xcom_pull(task_ids="train_and_log", key="high_card") or []

            run_id = cmp["run_id"]
            pipeline = mlflow_sklearn.load_model(f"runs:/{run_id}/model")
            preproc  = pipeline.named_steps["preproc"]
            reg      = pipeline.named_steps["reg"]

            df_train = pd.read_csv(f"{SHARED_TMP}/train.csv")
            df_train = df_train.drop(columns=high_card, errors="ignore")
            y_train  = df_train.pop("price")
            X_train  = df_train

            X_train_trans = preproc.transform(X_train)
            explainer = shap.KernelExplainer(reg.predict, X_train_trans[:MAX_SHAP])
            shap_values = explainer.shap_values(X_train_trans[:MAX_SHAP])

            shap.summary_plot(shap_values, X_train_trans[:MAX_SHAP],
                            feature_names=preproc.get_feature_names_out(),
                            show=False)
            import matplotlib.pyplot as plt
            plt.tight_layout()
            plt.savefig(f"{SHARED_TMP}/shap_summary.png", dpi=150, bbox_inches="tight")
            plt.close()

            mlflow.log_artifact(f"{SHARED_TMP}/shap_summary.png", artifact_path="shap")
            shap_uri = f"runs:/{run_id}/shap/shap_summary.png"
            ti.xcom_push("shap_uri", shap_uri)

        compute_shap = PythonOperator(
            task_id="compute_shap",
            python_callable=compute_shap_fn,
        )

        def record_history_fn(ti):
            cmp = ti.xcom_pull(task_ids="evaluate_and_promote", key="cmp")
            if not cmp:
                raise AirflowSkipException("No se encontró información de métricas (cmp) para record_history")

            run_name = ti.xcom_pull(task_ids="train_and_log", key="run_name")
            shap_uri = ti.xcom_pull(task_ids="compute_shap", key="shap_uri")

            engine = create_engine(CLEAN_DB_URI)
            with engine.begin() as conn:
                conn.execute(text(f"""
                    INSERT INTO {SCHEMA_CLEAN}.model_history (
                        experiment_id, model_name, run_id, run_name, model_version,
                        new_mse, new_rmse, new_mae, new_r2,
                        prod_mse, prod_rmse, prod_mae, prod_r2,
                        promoted, shap_uri
                    ) VALUES (
                        :exp_id, 'my_model', :run_id, :run_name, :mv,
                        :mse, :rmse, :mae, :r2,
                        :prod_mse, :prod_rmse, :prod_mae, :prod_r2,
                        :promoted, :shap_uri
                    )
                """), {**cmp, "run_name": run_name, "shap_uri": shap_uri})

        record_history = PythonOperator(
            task_id="record_model_history",
            python_callable=record_history_fn,
        )

        wait_for_clean >> ensure_history_table >> extract_data >> train_and_log >> \
            evaluate_and_promote >> compute_shap >> record_history 

  production_pipeline.py: |
    import os
    from datetime import datetime, timedelta

    import requests
    import pandas as pd
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from airflow.sensors.external_task import ExternalTaskSensor
    from airflow.exceptions import AirflowSkipException
    from sqlalchemy import create_engine, text
    from mlflow.tracking import MlflowClient

    # ─── Configuración ─────────────────────────────────────────────────────────────
    CLEAN_DB_URI    = os.getenv("CLEAN_DB_CONN")
    MLFLOW_MODEL    = "my_model"
    FASTAPI_HOOK    = "http://fastapi:8989/hooks/model_update"
    RAW_SCHEMA      = "raw_data"
    RAW_TABLE       = "inference_logs"

    default_args = {
        "owner": "airflow",
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    }

    with DAG(
        dag_id="production_pipeline",
        default_args=default_args,
        start_date=datetime(2025, 5, 1),
        schedule_interval="@hourly",
        catchup=False,
        tags=["production"],
    ) as dag:

        def ensure_rawdata_table_fn():
            engine = create_engine(CLEAN_DB_URI)
            with engine.begin() as conn:
                # Crear esquema
                conn.execute(text(f"CREATE SCHEMA IF NOT EXISTS {RAW_SCHEMA};"))
                # Crear tabla
                conn.execute(text(f"""
                    CREATE TABLE IF NOT EXISTS {RAW_SCHEMA}.{RAW_TABLE} (
                        id             SERIAL PRIMARY KEY,
                        requested_at   TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT NOW(),
                        model_name     TEXT    NOT NULL,
                        model_version  INTEGER NOT NULL,
                        run_id         TEXT    NOT NULL,
                        input_data     JSONB   NOT NULL,
                        prediction     JSONB   NOT NULL
                    );
                """))

        ensure_rawdata_table = PythonOperator(
            task_id="ensure_rawdata_table",
            python_callable=ensure_rawdata_table_fn,
        )

        wait_for_history = ExternalTaskSensor(
            task_id="wait_for_model_history",
            external_dag_id="modeling_pipeline",
            external_task_id="record_model_history",
            mode="reschedule",
            poke_interval=30,
            timeout=10*60,
        )

        def notify_fastapi_fn():
            client = MlflowClient()
            # Obtener la última versión en Production
            prod_versions = client.get_latest_versions(MLFLOW_MODEL, stages=["Production"])
            if not prod_versions:
                raise AirflowSkipException("No hay modelo en Production")

            latest = prod_versions[0]
            model_name    = latest.name
            model_version = int(latest.version)
            run_id        = latest.run_id

            payload = {
                "model_name":    model_name,
                "model_version": model_version,
                "run_id":        run_id
            }
            resp = requests.post(FASTAPI_HOOK, json=payload, timeout=10)
            resp.raise_for_status()
            print(f"✅ Notified FastAPI of new production model: {payload}")

        notify_fastapi = PythonOperator(
            task_id="notify_fastapi",
            python_callable=notify_fastapi_fn,
        )

        ensure_rawdata_table >> wait_for_history >> notify_fastapi 